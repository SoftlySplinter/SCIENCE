Artificial Nueral Networks
==========================

Linear Classification
---------------------

$h(\vec{x}) = sign(x)$

Uses an absolute threshold (e.g. binary).


Linear Regression
-----------------

Apply a linear function.

$h(\vec{x}) = s$


Logistic Regression
-------------------

Apply a 'soft' threshold (like a sigmoid function).

$h(\vec{x}) = \theta s$


Perception
----------

A type of ANN.

$(x_0, x_1, ..., x_d) = \vec{x} \rightarrow inputs, \vec{w} \rightarrow weights$

$$
o(\vec{x}) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if} \vec{w} \vec{x} > 0\\
    -1 & \quad \text{otherwise}
  \end{array} \right.
$$

$$
\sum^d_iw_ix_i = \vec{w}\vec{x}
$$

This can represent a logical function, e.g.: $x1 \&\& x2$

But can't represent some, e.g.: XOR.

$$
h(\vec{x}) = sign( \sum^d_i{w_i x_i} ) = sign(\vec{w}\vec{x})
$$

Each run pick a misclassified example:

$$
sign(\vec{w}\vec{x}^{(n)}) \neq y^{(n)}
$$

Update weight:

$$
\vec{w} \leftarrow \vec{w} + y^{(n)}\vec{x}^{(n)}
$$

```
until no misclassified results:
	pick misclassified result
	update weight
done
```

Modified

$$
\Delta\vec{w} = \eta(y-o)\vec{x}
$$

$o \rightarrow$ Perception output.

$\eta \rightarrow$ Small constant (learning weight).

Can be proved to converge with a linearly seperable data set and learning rate which is sufficiently small.


Linear Regression
-----------------

Target function $f: \vec{x} \rightarrow y \in \mathbb{R}$

$$
h(\vec{x}) = \vec{w}\vec{x} \text{ that approximates } f(\vec{x})
$$

By minimising

$$
E(\vec{w}) = \sum^{N}_{n-1}((\vec{w}\vec{x}_n)-y_n)^2
$$

$$
\nabla E(\vec{w}) = \frac{\partial E(\vec{w})}{\partial\vec{w}} = 2 \sum^{N}_{n-1}((\vec{w}\vec{x}_n)y_n)^2 \vec{x}_n = 0
$$

$$
\vec{w}^* = (\frac{1}{N}\sum^{N}_{n-1}\vec{x}_n\vec{x}_n)^{-1}(\frac{1}{N}\sum^{N}_{n-1}\vec{x}_n y_n)
$$


Logistic Regression
-------------------

$$
\theta(s) = \frac{e^s}{1 + e^s} = \frac{1}{1+e^{-s}}
$$

$$
h(\vec{x}) = \theta{s} \text{ can be interpeted as } P(x)
$$

e.g.:

$$
P(y|\vec{x}) = \left\{
  \begin{array}{l l}
    f(\vec{x}) & \quad \text{for } y = 1\\
    1 - f(\vec{x}) & \quad \text{for } y = -1
  \end{array} \right
$$

Learn:

$$
s(\vec{x}) = \theta(\vec{w}\vec{x}) \approx f(\vec{x})
$$

Error Measure:

For $(\vec{x}, y)$ $y$ is generated by $f(\vec{x})$.

If $h=f$, how likely to get $y$ from $\vec{x}$

$$
P(y|\vec{x}) = \left\{
  \begin{array}{l l}
  h(\vec{x}) & \quad \text{for } y = 1\\
  1 - h(\vec{x}) & \quad \text{for } y = -1
  \end{array} \right
$$

Likelyhood:

Substitute $h(\vec{x})$ for $\theta(\vec{w}\vec{x})$

Where $\theta(-s) = 1-\theta(s)$

$$
P(y|\vec{x}) = \theta(y\vec{w}\vec{x})
$$

Likelyhood of $D=<\vec{x}_1, y_1>, ..., <\vec{x}_N, y_N>$ is

$$
\prod^{N}_{n=1} P(y_n|\vec{x}_n) = \prod^{N}_{n=1}\theta(y_n \vec{w} \vec{x}_n)
$$

For convenience we change $max(likelyhood)$ to:

$$
min(\frac{-1}{N}ln\prod^{N}_{n=1}\theta(y_n \vec{w}\vec{x}_n))
$$

Simplify to:
$$
\frac{1}{N}\sum^{N}_{n=1}ln(\frac{1}{\theta(y_n\vec{w}\vec{x}_n))
$$

Gradient Descent
----------------

```
start a W(0)
step down steepest slope
fixed step size W(1) = W(0) + step_size * direction
```

Mathematically:

$$
W(1) = W(0) + \eta v
$$
